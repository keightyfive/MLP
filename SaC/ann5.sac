/*
name:	  ann5.sac
author: Kevin Klein
date:	  14/03/2017
descr.:	MLP (Multilayer Perceptron), also referred to as a feedforward artificial
 		    neural network, using Backpropagation for supervised learning
comp: 	sac2c -o ann4 ann4.sac
*/	

use StdIO: all;
use Array: all;
use CommandLine: all;
use Benchmarking: all;
//use RTClock: all;
//use RTimer: all;

//double getRand();
// SaC random function
external double random(double MIN, double MAX);
#pragma linkname "SACdrand"
#pragma linksign [0,1,2]
#pragma effect Rand::RandomGen

// 3 inputs nodes
#define numInputs 3
// 100 neurons in hidden layer
#define numHidden 100
// 4 sets of inputs paired with target outputs for XOR problem
#define numPatterns 4

int main()
{
	// number of training epochs
	numEpochs = 500000;
	// logistic regression values
	LR_IH = 0.7;
	LR_HO = 0.07;

	// variables
	patNum = 0;
	errThisPat = 0.0;
	outPred = 0.0;
	RMSerror = 0.0;

	// weights
	weightsIH = with{
                    ([0, 0] <= iv <= [numInputs,numHidden]): 0.0;
                  }: genarray([numInputs,numHidden], 0.0);                

	weightsHO = with{
                    ([0] <= iv <= [numHidden]): 0.0;
                  }: genarray([numHidden], 0.0);

	// supervised traing data (input sets with corresponding desired output)
	trainInputs = with{
                      ([0,0] <= iv <= [numPatterns,numInputs]): 0;
                    }: genarray([numPatterns,numInputs], 0);

  // target output 
	trainOutput = with{
                      ([1] <= iv <= [2]): 1;
                    }:genarray([numPatterns], -1);

	// init inputs
	trainInputs = reshape([numPatterns,numInputs], [-1,-1,-1, -1,1,1, 1,-1,1, 1,1,-1]);

	// get random values
	min = 0.1;
	max = 0.9;

	//************************************
	// init weights with random values
	for(j = 0; j < numHidden; j++)
	{
		weightsHO = modarray(weightsHO, [j], (random(min,max) - 0.5) / 2.0);
		
		// loop through no of inputs
		for(i = 0; i < numInputs; i++)
		{
			weightsIH = modarray(weightsIH, [i,j], (random(min, max) - 0.5) / 5.0);
		}	
	}

	interval = getInterval( "work", 0);
	start(interval);

	// train the network
    for(j = 0; j <= numEpochs; j++)
    {
    	for(i = 0; i < numPatterns; i++)
      {
        	minPatt = 0.0;
			    maxPatt = 4.0;

        	// select one of the input-output patterns for learning
        	patNum = toi(random(minPatt, maxPatt)) % numPatterns;

        	// matrix multiply using set-notation in with loop
          hiddenVal = with {
            (. <= [h,m] <= .): with {
              ([0] <= [k] < [shape(trainInputs)[[1]]]): tod(trainInputs[[patNum, k]]) * weightsIH[[k, m]];
            }: fold(+, 0.0);
          }: genarray([1, shape(weightsIH)[[1]]], 0.0);
          hiddenVal = reshape([numHidden], [hiddenVal]);

          // calculate the output of the network (set-notation)
          outPred = sum(hiddenVal[[i]] * weightsHO[[i]]);

     			// calculate the error
       		errThisPat = outPred - tod(trainOutput[patNum]);

          // update the weights from output layer to hidden layer
          weightsHO = with {
            (. <= v <= .): (weightsHO[v] - (LR_HO * errThisPat * hiddenVal[v]));
          }: modarray (weightsHO);           
/*
     				// regularisation on the output weights
      			if(weightsHO[z] < -5.0)
      			{
       				weightsHO[z] = -5.0;
      			}
      			else if(weightsHO[z] > 5.0)
      			{
       				weightsHO[z] = 5.0;
      			}
*/
          weightsIH = with {
            (. <= v <= .): (weightsIH[v] - (1.0 - (LR_IH * errThisPat)));
          }: modarray (weightsIH); 
	
		  }//for(i = 0; i < numPatterns; i++)

		  // calculate the overall error
    	RMSerror = 0.0;
    	for(p = 0; p < numPatterns; p++)
    	{
        	patNum = p;
        	RMSerror = RMSerror + (errThisPat * errThisPat);
    	}

    	RMSerror = RMSerror / tod(numPatterns);
    	//RMSerror = sqrt(RMSerror);

      printf("epoch = %d RMS Error = %f\n", j, RMSerror);
    	
	  }//for(j = 0; j <= numEpochs; j++)

	end(interval);

	// display results
  printf("---------------------------------- \n");
  for(i = 0; i < numPatterns; i++)
  {
  	patNum = i;
  	//trainNet();
   	printf("target output = %d, actual output = %f\n", trainOutput[patNum], outPred);
  }

	printResult(interval);

	return 0;
}
