/*
name:	  ann.sac
author: Kevin Klein
descr.:	MLP (Multilayer Perceptron), also referred to as a feedforward artificial
 		    neural network, using Backpropagation for supervised learning
comp: 	sac2c -o ann ann.sac
*/	

use StdIO: all;
use Array: all;
use CommandLine: all;
use Benchmarking: all;

// random function
external double random(double MIN, double MAX);
#pragma linkname "SACdrand"
#pragma linksign [0,1,2]
#pragma effect Rand::RandomGen

// number of input neurons
#define NO_INPUT_NEURONS 3
// number of neurons in hidden layer
#define NO_HIDDEN_NEURONS 100
// 4 sets of inputs paired with their target outputs in XOR problem
#define NO_PATTERNS 4

int main()
{
  // number of training epochs (iterations)
  NO_EPOCHS = 500;

	// logistic regression values for backpropagation
	LR_IH = 0.7;
	LR_HO = 0.07;

  // single output neuron
  actual_output = 0.0;
	// other variables
	pattern_no = 0;
	pattern_error = 0.0;
	rms_error = 0.0;

  // upper-lower random values
  min = 0.1;
  max = 0.9;

	// weights
	weights_IH = with{
                    ([0, 0] <= iv <= [NO_INPUT_NEURONS,NO_HIDDEN_NEURONS]): 0.0;
                  }: genarray([NO_INPUT_NEURONS,NO_HIDDEN_NEURONS], 0.0);                

	weights_HO = with{
                    ([0] <= iv <= [NO_HIDDEN_NEURONS]): 0.0;
                  }: genarray([NO_HIDDEN_NEURONS], 0.0);

	// supervised traing data (input sets with corresponding desired output)
	input_data = with{
                      ([0,0] <= iv <= [NO_PATTERNS,NO_INPUT_NEURONS]): 0;
                    }: genarray([NO_PATTERNS,NO_INPUT_NEURONS], 0);

  // target output 
	output_data = with{
                      ([1] <= iv <= [2]): 1;
                    }:genarray([NO_PATTERNS], -1);

	// init inputs
	input_data = reshape([NO_PATTERNS,NO_INPUT_NEURONS], [-1,-1,-1, -1,1,1, 1,-1,1, 1,1,-1]);

	// init weights with random values
	for(j = 0; j < NO_HIDDEN_NEURONS; j++)
	{
		weights_HO = modarray(weights_HO, [j], (random(min,max) - 0.5) / 2.0);
		
		// loop through no of inputs
		for(i = 0; i < NO_INPUT_NEURONS; i++)
		{
			weights_IH = modarray(weights_IH, [i,j], (random(min, max) - 0.5) / 5.0);
		}
	}

	interval = getInterval("work", 0);
	start(interval);

	// train the network
    for(j = 0; j <= NO_EPOCHS; j++)
    {
    	for(i = 0; i < NO_PATTERNS; i++)
      {
        	min_patt = 0.0;
			    max_patt = 4.0;

        	// select one of the input-output patterns for learning
        	pattern_no = toi(random(min_patt, max_patt)) % NO_PATTERNS;

        	// matrix multiply using set-notation in with loop
          values_hidden_layer = with {
            (. <= [h,m] <= .): with {
              ([0] <= [k] < [shape(input_data)[[1]]]): tod(input_data[[pattern_no, k]]) * weights_IH[[k, m]];
            }: fold(+, 0.0);
          }: genarray([1, shape(weights_IH)[[1]]], 0.0);
          values_hidden_layer = reshape([NO_HIDDEN_NEURONS], [values_hidden_layer]);

          print(values_hidden_layer);

          // calculate the output of the network (set-notation)
          actual_output = sum(values_hidden_layer[[i]] * weights_HO[[i]]);

     			// calculate the error
       		pattern_error = actual_output - tod(output_data[pattern_no]);

          // update the weights from output layer to hidden layer
          weights_HO = with {
            (. <= v <= .): (weights_HO[v] - (LR_HO * pattern_error * values_hidden_layer[v]));
          }: modarray (weights_HO);           
/*
     				// regularisation on the output weights
      			if(weights_HO[z] < -5.0)
      			{
       				weights_HO[z] = -5.0;
      			}
      			else if(weights_HO[z] > 5.0)
      			{
       				weights_HO[z] = 5.0;
      			}
*/
          weights_IH = with {
            (. <= v <= .): (weights_IH[v] - (1.0 - (LR_IH * pattern_error)));
          }: modarray (weights_IH); 
	
		  }//for(i = 0; i < NO_PATTERNS; i++)

		  // calculate the overall error
    	rms_error = 0.0;
    	for(p = 0; p < NO_PATTERNS; p++)
    	{
        	pattern_no = p;
        	rms_error = rms_error + (pattern_error * pattern_error);
    	}

    	rms_error = rms_error / tod(NO_PATTERNS);
    	//rms_error = sqrt(rms_error);

      printf("epoch = %d RMS Error = %f\n", j, rms_error);
    	
	  }//for(j = 0; j <= NO_EPOCHS; j++)

	end(interval);

	// display results
  printf("---------------------------------- \n");
  for(i = 0; i < NO_PATTERNS; i++)
  {
  	pattern_no = i;
  	//trainNet();
   	printf("target output = %d, actual output = %f\n", output_data[pattern_no], actual_output);
  }

	printResult(interval);

	return 0;
}
